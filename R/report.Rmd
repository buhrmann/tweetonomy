---
title: "Ideological twitter communities"
output: 
  html_document: 
    css: style-custom.css
    fig_caption: yes
    number_sections: yes
    self_contained: no
    theme: united
    toc: yes
---

```{r, echo=F, message=F, warning=F, cache=F}
#themes: "default", "cerulean", "journal", "flatly", "readable", "spacelab", "united", and "cosmo".
#rm(list=ls(all=T))
source('common.R')
library(printr)
library(scales)
library(pander)
library(DT)
```

This document gathers and illustrates work on a set of tools for analyzing twitter communities and their interaction with [hadoop](https://hadoop.apache.org) and [R](https://www.rstudio.com/).

# Pipeline overview
Data is gathered by using twitter's public [streaming API endpoint](https://dev.twitter.com/streaming/overview) to extract tweets related to Spanish political parties. A [Flume](https://flume.apache.org) agent with a [Twitter4j](http://www.twitter4j.org) source is used to funnel data received from Twitter's API onto disk. The custom Flume agent can be configured to follow a number of users, follow a number of keywords, and to filter by tweet language. The resulting tweets are stored on a distributed file system ([hdfs](https://en.wikipedia.org/wiki/Apache_Hadoop#Hadoop_distributed_file_system)) as raw json files. They are arranged in daily folders, with individual files containing a roughly equal number of tweets. [Hive](https://hive.apache.org) provides an [SQL-like](https://cwiki.apache.org/confluence/display/Hive/LanguageManual) view on these json files, with Hive tables being partitioned by day also. A number of hql scripts export daily edge-lists for mentions and retweets and other aggregated summaries into local text files. 

A number of different graphs can then be generated from the local data and inspected using the R library. Graphs are distinguished by layer (retweet, mentions or both combined) and version (e.g. one for the period of the catalan elections and one for the general elections). Graphs are also locally cached as R data files, so we don't have to re-create them for each analysis.

# Network level
Each graph layer consists of nodes representing twitter accounts, and edges between those nodes that capture the number of times a user A has retweeted another user B (in the retweet layer R), or how many times A has 'mentioned' B (in the mention layer M). First we will look at some overall statistics for the different graph layers.

```{r, echo=FALSE}
# Basic setup
version = "C"
layer = "R"
com_algo = "louvain"
num_coms = 10
```
In the following sections we'll use tweets from the `r versions[[version]]` election period as an example.

## Descriptive statistics
The following tables provide information about very general statistics of the three graph layers before and after basic preprocessing. The preprocessing consists in simplification of edges (collapsing parallel edges by summing their weights), filtering out edges with weights below some threshold (here 1 is selected, so no filtering), and by filtering out nodes not belonging to the largest (weakly) connected component (this removes nodes which ...). Lastly, stray nodes resulting from the filtering (those not connected to any other) are removed too.

```{r, echo=FALSE,cache=T}
# Get graphs
G = sapply(names(layers), function(x) { global_graph(x, version, com_algo, topn_coms=num_coms)}, simplify=FALSE)
df = t(sapply(G, function(x) basic_stats(x) ))
rownames(df) = layers[rownames(df)]
G = sapply(G, function(x) preprocess_graph(x, min_w=1), simplify=F)
dfp = t(sapply(G, function(x) basic_stats(x) ))
rownames(dfp) = layers[rownames(dfp)]
```


```{r, echo=FALSE, cache=F}
colnames(df) = gsub("_", " ", colnames(df))
colnames(dfp) = gsub("_", " ", colnames(dfp))
datatable(dfp, class="row-border stripe hover compact", rownames=T, filter="none",
          caption="Preprocessed graph", style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=F, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:ncol(dfp), digits=2, locale="de-DE")
```

Next we can plot the edge weight and node degree distributions of the (preprocessed) `r layers[layer]` layer:

```{r, echo=FALSE, cache=T, fig.height=3, fig.align="center", dev="svg", results="asis"}
wp = plot_weight_dist(G[[layer]], title=sprintf("Weight distribution (%s)", layers[layer]))
dp = plot_degree_dist(G[[layer]], title=sprintf("Degree distribution (%s)", layers[layer]))
grid.arrange(wp, dp, ncol=2)
cat("<p class='caption'>Figure: Log-log plots of network weight and degree distribution. </p>")
```

We can also find various node-centrality (importance) measures. E.g. here are the `r nact<-5; nact` most central twitter accounts with respect to in-degree (number of retweeters), and page-rank for the `r layers[layer]` layer:

```{r, echo=FALSE, cache=T, results="asis"}
centrs = c("indeg", "pgrank")
acts = lapply(centrs, function(x) topn_by_centrality(G[[layer]], centrality=x, topn=nact))
```
```{r, echo=FALSE, cache=F, results="asis"}
# pandoc.table(pp_int(acts[[1]]), caption=centrs[1], split.tables=Inf)
# pandoc.table(acts[[2]], caption=centrs[2], split.tables=Inf)
datatable(t(data.frame(acts[[1]])), class="row-border stripe hover compact", rownames=F, filter="none",
          caption=centrs[1], style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=T, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:length(acts[[1]]), digits=3, locale="de-DE")

datatable(t(data.frame(acts[[2]])), class="row-border stripe hover compact", rownames=F, filter="none",
          caption=centrs[2], style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=T, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:length(acts[[2]]), digits=3, locale="de-DE")
```

Note that the in-degree here corresponds to the number of _users_ having retweeted a particular account, not the number of retweets received (which requires taking into account the weight of each connection).

## Tweet frequency
To get a better feeling for the volume and frequency of tweets, an hql script aggregates the number of tweets per hour. Using these data we can plot the following time series:

TODO...

## Graph
TODO: Filtered down graph view...

# Community level
The goal of the analysis tools is to understand the interaction between different network communities. To this end, in each graph layer communities are identified based on structural network properties. For graphs as large as those explored here few community detection algorithms are sufficiently fast. In the following the "`r com_algo`"-method is used (TODO: explain), but others can be substituted too.

As a first step we can identify what level of modularity the community partitioning has achieved. In the case of the "`r com_algo`"-method applied to the `r layers[layer]` layer this is `r print(modularity(G[[layer]]$communities))`. TODO: explain this measure.

Next we check the number and size of communities identified.

* total number of communities: `r length(G[[layer]]$communities)`

```{r, echo=FALSE, cache=T, results="asis", fig.height=3, fig.align="center", dev="svg"}
plot_group_sizes(G[[layer]], grp_by="com", log_scale="", topn=50) + 
  xlab("Community index") + ylab("count") + ggtitle("50 largest communities")
```

On a log-scale this would likely be close to linear (indicating an exponential distribution of community sizes), as there are a few very large communities and a great number of small communities. In the presented form the communities are of little interest. What we're really interested in is the ideological identity of these communities. We identify them here based on the presence of certain individual accounts in each community. I.e. given a map that assigns certain groups of individuals to their corresponding ideological affiliation, we can represent communities by that affiliation rather than the abstract index in the previous figure. To this end we have simply compiled a list (by hand) of important twitter accounts associated with each political party in Spain. E.g. for the party Podemos, the group of accounts associated with the party itself (@ahorapodemos), their election candidate (@Pablo_Iglesias_), and their spokesperson (@ierrejon) are assigned the affiliation "podemos"; and similarly for all other parties. Having identified these lists of party-related accounts, we can then simply check if each member of a list is also part of a particular twitter community. If that is the case for all members of a list, the community is equated with that party. 

While in theory this doesn't guarantee that all communities, which, remember, are identified solely based on structural network properties, can be uniquely mapped to a party, in practice we found that to be the case. So in the following figure we display again the size of communities, but now with more meaningful identifiers based on political parties:

```{r, echo=FALSE, cache=T, results="asis", warning=F, message=F, fig.height=3, fig.align="center", dev="svg"}
plot_group_sizes(G[[layer]]) + scale_color_manual(values=party_colors)
```

Here, only those communities have been explicitly identified for which we have supplied the manual mapping of party affiliation. The remaining communities are subsumed under the label "unknown". Also, for efficiency reasons the mapping of community to party is only done for the n largest communities, usually 10, as there can be hundreds to thousands of smaller communities. Those accounts not belonging to the 10 biggest communities are filtered out for the rest of the analysis (there are in total `r sum(is.na(V(G[[layer]])$aff))` of these). We will also omit the group of "unknown" communities from further analysis, as well as remove accounts that end up isolated as the result of filtering out smaller and unknown communities.

## Community graph
Interaction matrix, 
collapsed graph, 
community colored and filtered graph,


## Comparison of communities
We can compare communities by calculating standard structural graph measures for the their subgraphs, i.e. subgraphs constituted by the nodes belonging to a community and all existing edges between them. The following table lists a number of these measure for each community (TODO: short explanation of each):

```{r, echo=FALSE, cache=T, results="asis", warning=T, message=F}
only_parties = function(g) {
  g %>% 
    filter_attr_in("aff", names(party_affiliations)) %>%
    filter_isolates()
}
G = sapply(G, only_parties, simplify=F) 
c = com_centralizations(G[[layer]], grp_by="aff")
```

```{r, echo=FALSE, cache=F, results="asis", warning=T, message=F}
colnames(c) = gsub("_", " ", colnames(c))

#set.alignment(default="left", row.names="left") #pandoc.table options
# pandoc.table(c, round=2, caption=sprintf("%s layer's community-wise centralization measures.", layer),
#             decimal.mark=",", big.mark=".", split.tables=Inf)

datatable(c, class="row-border stripe hover order-column compact no-ordering", rownames=T, filter="none",
          caption=sprintf("%s layer's community-wise centralization measures.", layer),
          options=list(info=F, page_length=10, paging=F, autowidth=F, searching=F, ordering=F, lengthChange=F),
          style="bootstrap") %>%
  formatNumbers(columns=1:ncol(c), digits=2, locale="de-DE")
```

Next we can pick out interesting community measures by identifying, for example, pairs with greatest spread (i.e. those best separating the communities, as measured by standard deviation), but little correlation (not measuring "the same thing", small absolute correlation).

```{r, echo=FALSE, cache=T, results="asis", warning=F, message=F}
corr = cor(c)
corr[upper.tri(corr, diag=T)] = NA # Remove duplicates from symmetric correlation matrix

# Which pairwise measures have the lowest correlation?
corr_molten = na.omit(melt(corr, value.name="Cor"))
corr_molten = corr_molten[order(abs(corr_molten$Cor)), ]
rownames(corr_molten) = NULL
colnames(corr_molten) = c("Measure 1", "Measure 2", "Correlation")
# Standard deviation of scaled measures: which measure spreads the groups out the most?
coeff_var = t(data.frame(apply(c, 2, sd) / apply(c, 2, mean)))
```

```{r, echo=FALSE, cache=F, results="asis", warning=F, message=F}
datatable(head(corr_molten, 10), class="row-border stripe hover order-column", rownames=F, 
          caption="10 measurement pairs with lowest absolute correlation", style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=F, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=3, digits=3, locale="de-DE")


datatable(coeff_var, class="row-border stripe hover order-column", rownames=F, 
          caption="Coefficient of variation (sd/mean)", style="bootstrap",
          options=list(info=F, page_length=2, paging=F, autowidth=T, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:ncol(coeff_var), digits=3, locale="de-DE")

```

```{r pairplot, echo=FALSE, cache=T, results="asis", warning=F, message=F, fig.height=4, fig.align="center", dev="svg"}
p1 = ggplot(c, aes(x=indeg, y=transitivity, label=rownames(c), color=rownames(c))) + 
  geom_point(size=3) + geom_text(vjust=1.5, hjust=1) +
  scale_x_continuous(expand=c(0.3, 0.0)) +
  scale_y_continuous(expand=c(0.1, 0.0)) +
  scale_color_manual(values=party_colors) +
  theme_minimal() + theme(legend.position="none") 

p2 = ggplot(c, aes(x=`max core`, y=outdeg, label=rownames(c), colour=rownames(c))) + 
  geom_point(size=3) + geom_text(vjust=1.5, hjust=1) +
  scale_x_continuous(expand=c(0.3, 0.0)) +
  scale_y_continuous(expand=c(0.1, 0.0)) +
  scale_color_manual(values=party_colors) +
  theme_minimal() + theme(legend.position="none") 

grid.arrange(p1, p2, ncol=2)
```

The two plots indicate parties along pairs of measure (from the tables above), that show low correlation but large spread. Not however, that some measures may be related to the size of the community, and since sizes are substantially different, might not be easily comparable.

<!---acts = important_actors_by_com(g, topn=10, centrality=centr, grp_by="aff")
centr = "indeg"
plot_important_actors(acts, centr)-->

# Hashtag level

