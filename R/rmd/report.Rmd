---
title: "Ideological twitter communities"
author: "Thomas Buhrmann, Ezequiel Di Paolo (report auto-generated `r Sys.Date()`)"
output: 
  html_document: 
    css: style-custom.css
    fig_caption: yes
    number_sections: yes
    self_contained: yes
    theme: united
    toc: yes
params:
  rebuild_cache: no
---

```{r libraries, echo=F, message=F, warning=F, cache=F}
#themes: "default", "cerulean", "journal", "flatly", "readable", "spacelab", "united", and "cosmo".
#rm(list=ls(all=T))
source('../lib/common.R', chdir=TRUE)
source('../lib/tweetfreq.R', chdir=TRUE)
source('../lib/hashtagutils.R', chdir=TRUE)
library(printr)
library(scales)
library(pander)
library(DT)

knitr::opts_chunk$set(cache.rebuild=params$rebuild_cache, cache=T, fig.width=12, fig.height=8, echo=FALSE, 
                      warning=FALSE, message=FALSE, fig.align="center", dev="svg")
```

This document gathers and illustrates work on a [set of tools](https://github.com/buhrmann/tweetonomy) for analyzing twitter communities and their interaction, built with [hadoop](https://hadoop.apache.org) and [R](https://www.rstudio.com/). The report is auto-generated by using scripts that first aggregate data in hadoop, and then invokes R tools that compile the markdown for this document while incorporating statistics, figures and tables generated from the data. The scripts are [here](https://github.com/buhrmann/tweetonomy/tree/master/hive/hql), and the markdown source for this report [here](https://github.com/buhrmann/tweetonomy/blob/master/R/rmd/report.Rmd).

# Pipeline overview
Data is gathered by using twitter's public [streaming API endpoint](https://dev.twitter.com/streaming/overview) to extract tweets related to Spanish political parties. A [Flume](https://flume.apache.org) agent with a [Twitter4j](http://twitter4j.org) source is used to funnel data received from Twitter's API onto disk. The [custom Flume source](https://github.com/buhrmann/cdh-twitter-example/tree/master/flume-sources/src/main/java/com/cloudera/flume/source) can be configured to [follow](https://dev.twitter.com/streaming/overview/request-parameters#follow) a number of users, [track](https://dev.twitter.com/streaming/overview/request-parameters#track) a number of phrases, and to filter tweets by  [language](https://dev.twitter.com/streaming/overview/request-parameters#language). An example configuration of the agent can be found [here](https://github.com/buhrmann/tweetonomy/blob/master/flume/twitter-parties.conf). Note that twitter account ids (instead of names) have to be used to follow users. 

The current strategy collects all tweets that originate in the official accounts of the major political parties as well as their official leaders and spokespersons; as well as those which mention or retweet any of these accounts. In addition, a number of keywords related to the elections are tracked. The result is filtered to only include tweets in Spanish.

The resulting tweets are stored on a distributed file system ([hdfs](https://en.wikipedia.org/wiki/Apache_Hadoop#Hadoop_distributed_file_system)) as raw json files. They are arranged in daily folders, with individual files containing a roughly equal number of tweets. [Hive](https://hive.apache.org) provides an [SQL-like](https://cwiki.apache.org/confluence/display/Hive/LanguageManual) view on these json files, with Hive tables being partitioned by day also. Various [hql scripts](https://github.com/buhrmann/tweetonomy/tree/master/hive/hql) export daily edge-lists for mentions and retweets and other aggregated summaries into local text files. 

A number of different graphs, and tables can then be generated from the local data and inspected using the [R tools](https://github.com/buhrmann/tweetonomy/tree/master/R). Graphs are distinguished by layer (retweet, mentions or both combined) and version (e.g. one for the period of the catalan elections and one for the general elections). Graphs are also locally cached as R data files, so they don't have to be re-created for each analysis (unless new data needs to be incorporated).

# Network level analysis
Each graph layer consists of vertices representing twitter accounts. Edges between those vertices capture the number of times a user A has retweeted another user B (in the retweet layer R), or how many times A has 'mentioned' B (in the mention layer M). First we will look at some overall statistics for the different graph layers.

```{r setup}
# Basic setup
version = "C"
layer = "R"
com_algo = "louvain"
num_coms = 10
```
In the following sections we'll use tweets from the `r versions[[version]]` election period as an example.

## Tweet frequency and volume
```{r freqs, results="asis"}
freqs = all_day_counts(from=periods[[version]]$from, to=periods[[version]]$to)
tweets_total = sum(freqs$freq) #
from = min(freqs$datetime)
to = max(freqs$datetime)
```
We start off by looking at the volume and frequency of tweets using an hql script that aggregates the number of tweets per hour. In total, for the **`r versions[[version]]`** version of the graph, we have **`r pp_int(sum(freqs$freq))`** tweets in the period between **`r pprint_date(from)`** and **`r pprint_date(to)`** (**`r ceiling(as.numeric(to-from))` days**).

For a more detailed view we can look at tweet frequencies over time, at different levels of resolution:

```{r freqs_out, results="asis", fig.height=3, fig.width=8}
# Aggregate by day
dayfreq = freqs %>%
  group_by(datetime=as.Date(datetime)) %>%
  summarize(freq = sum(freq)) %>%
  plot_tweet_freq(points=F) + xlab("Daily")
# And by hour
hourfreq = freqs %>%
  filter(datetime >= "2015-10-01" & datetime <= "2015-10-07") %>%
  plot_tweet_freq(points=F) + xlab("Hourly")

grid.arrange(dayfreq, hourfreq, ncol=2)
```

On the left tweets counts are shown aggregated by day and for the complete period. On the right an hourly count is shown for a single week.

## Descriptive statistics
The following tables provide information about very general statistics of the three graph layers before and after basic preprocessing. The preprocessing consists in 

* simplification of edges, i.e. collapsing parallel edges by summing their weights (counts of number of retweets or mentions). Though in theory there shouldn't be any parallel edges to begin with.
* filtering out edges with weights below some threshold (here 1 is selected, so no filtering) 
* filtering out vertices not belonging to the largest (weakly) connected component (this removes parts of the network that are disconnected from the main component). 

Lastly, stray nodes resulting from the filtering (those not connected to any other) are removed too.

```{r load_graphs}
G = sapply(names(layers), function(x) { global_graph(x, version, com_algo, topn_coms=num_coms,
                                                     recache=params$rebuild_cache, verbose=F)}, simplify=FALSE)
df = t(sapply(G, function(x) basic_stats(x) ))
rownames(df) = layers[rownames(df)] 
G = sapply(G, function(x) preprocess_graph(x, min_w=1), simplify=F)
dfp = t(sapply(G, function(x) basic_stats(x) ))
rownames(dfp) = layers[rownames(dfp)]
```
```{r basic_stats, cache=F}
colnames(df) = gsub("_", " ", colnames(df))
colnames(dfp) = gsub("_", " ", colnames(dfp))
datatable(df, class="row-border stripe hover compact", rownames=T, filter="none",
          caption="Raw graph", style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=F, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:ncol(df), digits=2, locale="de-DE")

datatable(dfp, class="row-border stripe hover compact", rownames=T, filter="none",
          caption="Minimally preprocessed graph", style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=F, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:ncol(dfp), digits=2, locale="de-DE")
```

Note that the total number of tweets from the previous section (**`r pp_int(sum(freqs$freq))`**), is not identical to the sum of retweet and mention edges or tweets. This is because retweets and mentions are not mutually exclusive. Retweets, for example, can also mention both the retweeted as well as other accounts. Equally a tweet may mention one or more accounts without being a retweet. As a result, the number of retweets only may be smaller than the total number of all tweets, as there will likely be mentions that are not retweets. On the other hand, the number of mentions may actually be greater than the actual total of tweets (e.g. if the average number of mentions per tweet is greater than 1). Also note that for the combined layer, the _weights_ of edges (#tweets) between identical pairs of accounts in the retweet and mention layers are simply added, so the corresponding "tweet" numbers do add up. In contrast, the combined _number_ of edges corresponds to the union of edges in both layers, so the number of edges do not necessarily add up. One layer may in fact be a subset of the other. E.g. in most cases it seems that if one user has retweeted another, then they will also have mentioned that other user. The converse is not generally true. Many users seem to mention others without ever retweeting them. As a result the set of connections formed by mentions may already contain all retweet connections, but not the other way round.

Next we can plot the edge weight and node degree distributions of the (preprocessed) `r layers[layer]` layer:

```{r basic_dist, fig.height=3, results="asis"}
wp = plot_weight_dist(G[[layer]], title=sprintf("Weight distribution (%s)", layers[layer]))
dp = plot_degree_dist(G[[layer]], title=sprintf("Degree distribution (%s)", layers[layer]))
grid.arrange(wp, dp, ncol=2)
cat("<p class='caption'>Figure: Log-log plots of network weight and degree distribution. </p>")
```

It's is clear that both distributions follow a power law (as common in small-world networks, for example).

We can also find various node-centrality (importance) measures. E.g. here are the `r nact<-5; nact` most central twitter accounts with respect to in-degree (number of retweeters) and page-rank for the `r layers[layer]` layer:

```{r graph_centr, results="asis"}
centrs = c("indeg", "pgrank")
acts = lapply(centrs, function(x) topn_by_centrality(G[[layer]], centrality=x, topn=nact))
```
```{r graph_centr_out, results="asis", cache=F}
# pandoc.table(pp_int(acts[[1]]), caption=centrs[1], split.tables=Inf)
# pandoc.table(acts[[2]], caption=centrs[2], split.tables=Inf)
datatable(t(data.frame(acts[[1]])), class="row-border stripe", rownames=F, filter="none",
          caption=centrs[1], style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=T, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:length(acts[[1]]), digits=3, locale="de-DE")

datatable(t(data.frame(acts[[2]])), class="row-border stripe", rownames=F, filter="none",
          caption=centrs[2], style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=T, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:length(acts[[2]]), digits=3, locale="de-DE")
```

Note that the in-degree here corresponds to the number of _users_ having retweeted a particular account, not the number of retweets received (which requires taking into account the weight of each connection).

## Graph
Plotting a graph with hundreds of thousands of nodes and millions of edges usually results in an unintelligeable "hairball". We may however, filter the graph down to more manageable size, for example by looking at a daily snapshot.
```{r day_graph}
day_str = "20150927"
dg = day_graph(day_str, layer, version, com_algo, G=G[[layer]]) %>%
    filter_attr_in("aff", names(party_affiliations)) %>%
    filter_min_weight(2) %>%
    filter_isolates()
deg = degree(dg)/max(degree(dg))
V(dg)$color = gray(1-(0.2+0.8*deg), alpha=NULL)
```
The following figure shows the graph for the day of **`r pprint_date(filename_to_date(day_str))`**:
```{r day_graph_out, results="asis"}
top_acts = topn_by_centrality(dg, centrality="indeg", topn=10)
incl_verts = which(V(dg)$name %in% names(top_acts))
V(dg)$label = NA
V(dg)[incl_verts]$label = V(dg)[incl_verts]$name

dgpl=g_plot(graph=dg, layout=layout_with_lgl(dg), colorAttr=NULL)
```
The visual graph representation is not particularly informative. The ten accounts with the highest in-degree are highlighted by their names. The unequal distribution of retweets is obvious from the fact that there are few nodes with high degree (encoded in the figure by vertex size) and many with significantly smaller degree.

# Community level
The goal of the analysis tools is to understand the interaction between different network communities. To this end, in each graph layer communities are identified based on structural network properties. For graphs as large as those explored here, few community detection algorithms are sufficiently fast. In the following the "`r com_algo`"-method is used ([see here e.g.](http://igraph.org/r/doc/cluster_louvain.html)), but others can be substituted too.

Eventually, we want to be able to follow community dynamics on a daily basis. But if we detect communities based on the graph of tweets for a particular day only, then the composition of communities may vary a lot from day to day. This is because an individual's retweet and mention activity may be related mostly with one particular group of fellow tweeters one day, but a different group another day. However, if we take into account all tweets for the whole period considered, this should give us a more stable picture of which community an individual user generally belongs to. Hence community assignment is based on the graph of all tweets. When later analyzing activity on a per-day basis, instead of re-calculating communities for the smaller day-only network, we simply assign each node that day to the community it belongs to in the "global" graph.

As a first step we can identify what level of [modularity](http://www.inside-r.org/packages/cran/igraph/docs/modularity) the community partitioning has achieved. In the case of the "`r com_algo`"-method applied to the **`r layers[layer]`** layer this is **`r round(modularity(G[[layer]]$communities), 3)`**.

Next we check the number and size of communities identified. There are in total **`r pp_int(length(G[[layer]]$communities))`** communities. Their sizes are distributed as follows (only the 25 biggest are shown and ordered by size):

```{r grp_size, results="asis", fig.height=3}
plot_group_sizes(G[[layer]], grp_by="com", log_scale="", topn=25) + 
  xlab("Community index") + ylab("count") + ggtitle("50 largest communities")
```

On a log-scale this would likely be close to linear (indicating an exponential distribution of community sizes), as there are a few very large communities and a great number of small communities. In the presented form the communities are of little interest. What we're really interested in is the ideological identity of these communities. We identify them here based on the presence of certain individual accounts in each community. I.e. given a map that assigns certain groups of individuals to their corresponding ideological affiliation, we can represent communities by that affiliation rather than the abstract index in the previous figure. To this end we have simply compiled a list (by hand) of important twitter accounts associated with each political party in Spain. E.g. for the party Podemos, the group of accounts associated with the party itself (@ahorapodemos), their election candidate (@Pablo_Iglesias_), and their spokesperson (@ierrejon) are assigned the affiliation "podemos"; and similarly for all other parties. Having identified these lists of party-related accounts, we can then simply check if each member of a list is also part of a particular twitter community. If that is the case for all members of a list, the community is equated with that party. 

While in theory this doesn't guarantee that all communities, which, remember, are identified solely based on structural network properties, can be uniquely mapped to a party, in practice we found that to be the case. So in the following figure we display again the size of communities, but now with more meaningful identifiers based on political parties:

```{r grp_size_aff, results="asis", fig.height=3}
plot_group_sizes(G[[layer]]) + scale_color_manual(values=party_colors)
```

Here, only those communities have been explicitly identified for which we have supplied the manual mapping of party affiliation. The remaining communities are subsumed under the label "unknown". Also, for efficiency reasons the mapping of community to party is only done for the n largest communities, usually 10, as there can be hundreds to thousands of smaller communities. Those accounts not belonging to the 10 biggest communities are filtered out for the rest of the analysis (there are in total `r pp_int(sum(is.na(V(G[[layer]])$aff)))` of such accounts). We will also omit the group of "unknown" communities from further analysis, as well as remove accounts that end up isolated as the result of filtering out smaller and unknown communities.

## Important community members
With community identities in place, we can determine the important members in each. E.g. the following tables show the 5 members with the highest in-degree (#retweeters) for the 6 biggest communities:

```{r imp_acts, results="asis"}
imp_acts = important_actors_by_com(G[[layer]], topn=5, centrality='indeg', grp_by="aff")
```
```{r imp_acts_out, results="asis", cache=F}
for (i in 1:6) {
  pandoc.table(pp_int(imp_acts[[i]]), caption=names(imp_acts)[i], split.tables=Inf)
}
```

## Community coherence and interaction

```{r only_parties, results="asis"}
only_parties = function(g) { 
  g %>% 
    filter_attr_in("aff", names(party_affiliations)) %>%
    filter_isolates()
}
G = sapply(G, only_parties, simplify=F) 
c = com_centralizations(G[[layer]], grp_by="aff")
```
Apart from the modularity measure given above, we can check how coherent the different communities are by measuring for each community the proportion of edges between members of the same community, and the proportion of edges members share with another community. If a community mostly retweets or mentions other accounts within the same community, then the corresponding value in the diagonal in the following interaction matrix should be large (close to 1), and the off-diagonal values low (close to 0):

```{r int_mat, results="asis"}
im = round(interaction_matrix(G[[layer]], grp_by="aff", scale=T), 2) 
```
```{r int_mat_out, results="asis", fig.height=4}
plot_interaction_matrix(im) + xlab("retweeted") + ylab("retweeter")
```

Another way to look at the above interactions is to plot a graph in which all nodes belonging to the same community are collapsed into one, with edges between communities being the result of adding up all individual edges between them:

```{r contr_plot_out, results="asis", fig.width=7} 
cols = adjust_hsv(party_colors, "s", 0.6)
cols = adjust_hsv(cols, "v", 1.4)
plot_contracted_by(G[[layer]], "aff", cols)
```

Here vertices are colored by their "official" party color, while their size corresponds to their degree and edge width is proportional to the number of tweets between communities.

## Comparison of communities
We can compare communities by calculating standard structural graph measures for the their subgraphs, i.e. subgraphs constituted by the nodes belonging to a community and all existing edges between them. The following table lists a number of these measure for each community (TODO: short explanation of each):

```{r party_centr_out, results="asis", cache=F}
colnames(c) = gsub("_", " ", colnames(c))
#set.alignment(default="left", row.names="left") #pandoc.table options
# pandoc.table(c, round=2, caption=sprintf("%s layer's community-wise centralization measures.", layer),
#             decimal.mark=",", big.mark=".", split.tables=Inf)

datatable(c, class="row-border stripe hover order-column compact no-ordering", rownames=T, filter="none",
          caption=sprintf("%s layer's community-wise centralization measures.", layers[[layer]]),
          options=list(info=F, page_length=10, paging=F, autowidth=F, searching=F, ordering=F, lengthChange=F),
          style="bootstrap") %>%
  formatNumbers(columns=1:ncol(c), digits=2, locale="de-DE")
```

Next we can pick out interesting community measures by identifying, for example, pairs with greatest spread (i.e. those best separating the communities, as measured by standard deviation), but little correlation (not measuring "the same thing", small absolute correlation).

```{r com_measures, cache=F}
corr = cor(c)
corr[upper.tri(corr, diag=T)] = NA # Remove duplicates from symmetric correlation matrix

# Which pairwise measures have the lowest correlation?
corr_molten = na.omit(melt(corr, value.name="Cor"))
corr_molten = corr_molten[order(abs(corr_molten$Cor)), ]
rownames(corr_molten) = NULL
colnames(corr_molten) = c("Measure 1", "Measure 2", "Correlation")
# Standard deviation of scaled measures: which measure spreads the groups out the most?
coeff_var = t(data.frame(apply(c, 2, sd) / apply(c, 2, mean)))
```
```{r com_measures_out, results="asis", cache=F}
datatable(head(corr_molten, 10), class="row-border stripe hover order-column", rownames=F, 
          caption="10 measurement pairs with lowest absolute correlation", style="bootstrap",
          options=list(info=F, page_length=10, paging=F, autowidth=F, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=3, digits=3, locale="de-DE")


datatable(coeff_var, class="row-border stripe hover order-column", rownames=F, 
          caption="Coefficient of variation (sd/mean)", style="bootstrap",
          options=list(info=F, page_length=2, paging=F, autowidth=T, searching=F, ordering=F, lengthChange=F)) %>%
  formatNumbers(columns=1:ncol(coeff_var), digits=3, locale="de-DE")

```

The following two plots indicate parties along pairs of measures (from the tables above), that show low correlation but large spread. Note however, that some measures may be related to the size of the community, and since sizes are substantially different, might not be easily comparable:

```{r pairplot, results="asis", fig.height=4}
p1 = ggplot(c, aes(x=indeg, y=transitivity, label=rownames(c), color=rownames(c))) + 
  geom_point(size=3) + geom_text(vjust=1.5, hjust=1) +
  scale_x_continuous(expand=c(0.3, 0.0)) +
  scale_y_continuous(expand=c(0.1, 0.0)) +
  scale_color_manual(values=party_colors) +
  theme_minimal() + theme(legend.position="none") 

p2 = ggplot(c, aes(x=`max core`, y=outdeg, label=rownames(c), colour=rownames(c))) + 
  geom_point(size=3) + geom_text(vjust=1.5, hjust=1) +
  scale_x_continuous(expand=c(0.3, 0.0)) +
  scale_y_continuous(expand=c(0.1, 0.0)) +
  scale_color_manual(values=party_colors) +
  theme_minimal() + theme(legend.position="none") 

grid.arrange(p1, p2, ncol=2)
```

## Graph
We can now also plot a more useful (filtered) version of the complete graph, taking into account community membership. Here, for example, is the graph with edges removed that have weights smaller than 6, and then nodes with degree smaller than 3. Nodes are coloured by political party, and a layout is used to place nodes that explicitly separates the communities.

```{r com_graph, results="asis"}
pg = G[[layer]] %>%
  filter_min_weight(6) %>%
  filter_min_degree(3) %>%
  filter_isolates()

cols = adjust_hsv(party_colors, "s", 0.6)
cols = adjust_hsv(cols, "v", 1.4)
vcols = as.character(cols[V(pg)$aff])
V(pg)$color = vcols

imp_acts = ia2df(important_actors_by_com(pg, topn=1, centrality='indeg', grp_by="aff"))
ia_flat = unlist(imp_acts, use.names=F)
incl_verts = which(V(pg)$name %in% ia_flat)
V(pg)$label = NA
V(pg)[incl_verts]$label = V(pg)[incl_verts]$name
```
```{r com_graph_out, results="asis",fig.width=8}
l = layout_modular_grid(pg, grp_by="aff", width=1, weight_fun=function(x) x, layout=layout_with_kk)
#l = layout_modular_grid(pg, grp_by="aff", width=1, weight_fun=NULL, layout=layout_with_lgl)
pg  = g_plot(pg, layout=l, colorAttr=NULL, emph_ids=incl_verts) 
```

<!---acts = important_actors_by_com(g, topn=10, centrality=centr, grp_by="aff")
centr = "indeg"
plot_important_actors(acts, centr)-->

# Hashtag level
As a first step towards understanding the _dynamics_ and _interaction_, rather than merely the structure of communities, we can analyze the frequency with which each community uses a given hashtag, dynamically, over time. I.e. for a given hashtag we can plot the time series given by the number of retweets for each community that use the given hashtag on a per-day basis.

```{r freq_graph, results="asis"}
if (params$rebuild_cache) {
  freqs = cache_all_tagfreqs(layer=layer, version=version, G[[layer]])
} else {
  freqs = get_cached("tagfreqs", tagfreqs_fnm[[layer]])
}
tags_df = tag_all_df(freqs)
```
```{r freq_graph_out, results="asis"}
# tags_df %>%
#   group_by(tag) %>%
#   summarise(freq = sum(Freq)) %>%
#   arrange(desc(freq)) %>%
#   print(n=50)

plot_tagfreq(tags_df, "pge2016")
```

The ultimate goal here is to compare the hashtag-frequency time series for different communities to determine their mutual influence. The question currently is whether at a sufficient resolution (e.g. hourly) there are enough tweets for a hashtag to perform information-theoretic analysis of the time series.

# Media consumption
We identify media outlets of interest by manual inspection of the top 100 accounts in each community and creating the set of those which belong to tv, press or other media companies. Note (TODO): since we have previously filtered out all accounts not belonging to one of the big communities, some media outlets, namely those not belonging obviously to a community, won't appear here. THis should probablt be changed.

The first plot shows the important media outlets, as well as the communities that they're mostly affiliated with:

```{r media}
media = get_media(G[[layer]])
med_aff = media_affiliation(G[[layer]], media)
med_col = cols[as.character(med_aff$affiliation)]
names(med_col) = names(media)
```
```{r media_out}
dotline_plot(named_to_df(media), "name", "val", xlab_s=1.25, labels=med_aff$aff) + ylab("degree") +
  scale_color_manual(values=med_col) + expand_limits(y=c(0, max(media)*1.1))
```

However, this doesn't give us the whole picture. Although a media outlet may be mostly associated with one particular community (that retweets its news the most, for example), other communities may also interact with it. In the following figure, we therefore plot for each community the proportion of their tweets that are retweets from a given media outlet.

```{r media_out_2}
grp_med = grp_media_affiliations(G[[layer]], media, "aff")
dotline_arrange_df(grp_med)
```

An interesting observation is that in some cases, particular communities focus their tweet activity mostly on one or a few specific media outlets, while others divide their attention more equally. We can try to capture this quantitatively too.